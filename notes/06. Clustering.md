Clustering
* тип unsupervised learning
	* начин на обучение без "учител"
	* не подаваме очаквани резултати (т.е. не знаем отговори предварително)
	* опитваме се да разберем нещо без да знаем каква е реалната обстановка
	* състоите се от 2 основни типа алгоритми
		* Clustering
		* Dimensionality reduction (намаляване на размерността) a.k.a feature selection
			* дава възможност да изберем по-добри данни за трениране
* Finding structures in data
* тук имаме хипотеза че данни са в групи
	* подобните данни са близо разположени
* клъстерирнето нормално работи в много измерения
	* недостатък е че нямаме почти никаква възможност да видим как се справя
* clustering anlysis
	* finding clumps in data
* Приложения
	* Намиране на аномалии (Anomaly detection)
	* В медицината: Класифициране на различни типове тъкани
	* Маркетинг: Групиране на подобни продукти
	* Image segmentation, Object recoginition
	* Анализ на graphs
		* Social network analysis
	* Crime analysis: "hot" spatial data, similar crimes
		* престъпления които са свързани
	* може да се използва навсякъде където имаме някакви видове групи (като ние не знаем тези групи какви са)
		* ако знаем какви са групите, ни трябва класификация
* В основния случай имам някакво групиране във времето или пространството
* k-means clustering
	* основния алгоритъм който се използва
	* базиран на прототипи
		1. Подаваме данните на модела
		2. Избираме си k което е брой клъстери (купчинки)
		* Всяка една купчинка има определена характеристика
			* ние можем да покажем по 1 член от всяка купчинка т.е. да дадем пример на модела всяка една купчинка къде се намира
			* обаче може да не знаем от коя купчинка са тези точки
		3. ние даваме прототипи на модела (по 1 точка от всяка купчинка)
			* в общия случай избираме "k" на брой случайни точки от данните за прототипи
		* предположение: разстоянието от центъра на купчинката до една точка ще бъде малко ако тя принадлежи на тази купчинка, но ще бъде голямо ако тя не принадлежи на купчинката
		4. Избираме нов център
			* средното на min и max за всяко едно измерение
		5. Пак изчисляваме разстоянията от всеки центъра до всяка една точка
		* В момента в който новите клъстери съвпаднат с предните клъстери алгоритъма приключва
	* Ако ние подадем "k" на брой точки (по една за всяка купчинка) алгоритъма винаги ще връща един и същи резултат, при случайно избрани от модела "к" на брой точки нямаме такава гаранция
	* В зависимост от мярката за разстояние може да получим различни резултати
	* Минимизираме разстоянието от всяка една точка до нейния център
	* Предпочита купчинки които са симетрични
		* за това е хубаво да скалираме данните (нормализираме данните) преди да ги подадем на модела
	* Работи за линейно разделими clusters (да може данните да бъдат разделени с права)
	* Ако имаме препокриване на клъстери k-means не работи добре
	* Ако има структура която не сме очаквали или няма никаква структура, ние не разбираме (особено при многомерни пространства)
	* Random initial seed => may lead to poor performance
		* if the initial points aren't placed well enough or if the cluster are too mixed
	* K-means++
		* default-ен начин на инициализация за k-means алгоритъма в sklearn
		* uses centers which are far away from each other
			* instead of random initialization
		* alorithm steps
			* Choose the first centroid uniformly at random
			* To choose the nex centroids, use a weighted probability distribution
				* Based on all currently selected centroids
				* Further away => greater probability
			* After all centroids have been initialized, proceed as usual
	* нормално грешките стават по границите
		* може да вземем данните по границите и да ги разгледаме допълнително за да разберем дали модела работи правилно
	* Как да изберем числото "K" ако не знаем броят на клъстерите?
		* elbow method - graphical
		* Inertia is a measure of clustering qualitys
			* Like grid search, initialize KMeans with a range of "k" values
			* Fit and calculate the inertia (given by default in sklearn)
			* Plot inertia vs number of clusters
			* Find the "elbow point" of the plot - this is the optimal "k"
				* Inertia always decreases but some models ovefit the data
				* Ако функцията намалява все по-плавно и ни липсва "лакътя" не е добро разделяне по принцип
				* Ако нямаме рязък спад на графиката, това може да ни подскаже че алгоритъма е неподходящ
				* Така се дебъгва gradient decent, така се дебъгват невронни мрежи и още много неща
					1. Имаме мярка която трябва да спада постоянно
					2. Търсим рязка граница
						* Ако има много рязка граница при която спада, значи тва е оптималната граница
						* Ако няма такава (т.е. границата е сравнителни плавна) е много вероятно да сме сбъркали нещо (имаме грешно предположение)
	* До колко клъстерирането работи? (Evaluating Clustering Quantity)
		* silhouette analysis (Силуетен анализ)
* Hierarchical Clustering (Agglomerative Clustering)
	* Another prototype-based clustering
	* хипотеза: близките точки съдържат йерархия (т.е. може да ги подпредим в дърво)
	* хваща близките точки, независимо в каква форма са, докато k-means хваща равномерно разпределни точки (т.е. сферични купове)
	* linkage - функцията която описва как се свързват клъстерите
	* има fit, fit_predict методите но няма "predict"
		* няма как да тренираме за едни точки и да тестваме за други
	* при нова точка трябва да обучим наново модела
	* можем да направим дендограма (dendogram) - графика която прилича на дърво и показва резултатите от клъстерирането
		* показва кои точки (индекси) са били свързани на всяка една стъпка
		* добре е отделните клъстери да има д/г еднакви дендограми
			* ако са подобни това означава че имаме д/г равен брой елементи във всички от тях
			* ако не са подобни, някои доминират (може да се използва за детекция на аномалии)
* за unsupervised learning в 90% от случаите k-means и Hierarchical Clustering ни вършат работа
* DBSCAN (Density-Based Spatial Clustering of Applications with Noise)
	* тип клъстериране със шум
	* нормалните точки имат висока плътност
	* шумът е случаен и има ниска плътност
	* подобно на регресия с outliers и RANSAC може да разпознава кои са outliers и не работи с тях
		* както регресията с outliers не дава резултати за outlier-ите, работеше само с inliers с високи резултати, така и тази DBSCAN ще ни позволи да махнем някои точки, но няма да получим cluster за тях
	* label all points as "score points" or "noise points"
		* core point has at least "m" points with radius "r"
	* use core points to create clusters
	* DBSCAN vs. k-means
		* noise points are not assigned to any cluster
		* doesn't assume spherical shape
	* Disadvantages
		* "curse of dimensionality"
			* ако имаме много на брой измерения, става все по-вероятно
				* точки които са подобни да се окажат на голямо разстояние
				* точки които не са подобни да се окажат на близко разстояние
			* при много на брой измерения
				* имаме проблеми със смятането на разстояния и отделянето на разстояния (клъстериране)
		* the hyperparameters need to be optimized
	* параметри
		* eps - разстояние
			* колкото е по-голям, толкова повече точки ще са inliers
		* min_samples - минимален брой наблюдения във всеки клъстер
		
Допълнителни бележки:
* Reinforcement learning
	* Възможността да учим от предишен опит
	* Използва се при играене на игри.
	* примери
		* AlphaGO
		* самоуправляващи се коли
* isomap - да намерим структура в точките
* важно е при всеки един модел да си нормализираме данните
* преди клъстериране може да направим dimensionality reduction
	* клъстерирането с по-малко измерения може да работи по-добре
* може да си трансформираме данните така че моделите ни да се обучават по-добре
* Кога една задача е за клъстериране и кога за класификация?
	* класификация - ако знаем labels
	* клъстериране - ако нямаме labels или ги игнорираме
		* може да игнорираме labels и да търсим до колко cluster-ите отговарят на оригиналните labels
		* ако нямаме labels, след клъстериране получаваме клъстери, index-а на клъстер-а може да бъде клас (т.е. може да направим класификация)
		* т.е. може да използваме едно след друго "класификация" и "клъстериране"
		* дори може да ги съчетаем в ensemble
* Най-голяма сила имаме когато комбинираме алгоритми
	* върху едни и същи данни пускаме различни алгоритми
	* върху различни данни пускаме един и същ алгоритъм
* Трябва да знаем кой тип алгоритъм кога е подходящ
* SVMs като цяло дават по-добри резултати от Logistic Regression, по-сложни само
* Random Forest по принцип дава по-добри резултати от всички останали
* AdaBoost по идея трябва да дава по-добри резултати защото са няколко комбинирани модела
* Може да използваме няколко модела в един ensemble и да искаме от тях majority vote
* Всички алгоритми за unsupervised learning, работят на база на предположения, тези предположения винаги са bias
	* всяко едно клъстериране работи със разстояния (разстоянието е предположение)
* Повече измерения (повече колонки в dataset-a) по принцип води до по-добри резултати
	* може да вземем оригиналните колонки и да направим polynomial features и да ги увеличим
* Sentiment analysis

Resources:
* [Comparing different clustering algorithms on toy datasets](http://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_comparison.html)
* [Combining Clustering and Classification for Ensemble Learning](https://arxiv.org/abs/1708.08591)
* [30 Questions to test a data scientist on K-Nearest Neighbors (kNN) Algorithm](https://www.analyticsvidhya.com/blog/2017/09/30-questions-test-k-nearest-neighbors-algorithm/)
