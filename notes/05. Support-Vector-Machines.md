Бележки
* Feature engeneering - опит за извеждане на нови колонки от данните
* Overfitting - слабо предсказване на нови данни
* Boosting - комбиниране на няколко слаби модела
* Нормализацията подпомага стабилността на модела
* SVM (Support Vector Machines) - метод на опорните вектори
    * Идеята е да намерим правата която разделя класовете (която, обратно на линейната регресия, увеличава разстоянието)
    * Трябва да се избере правата която най-добре разделя класовете
        * при положение че те могат да бъдат разделени с права
        * ако не могат да бъдат разделени с права, тази задача става невъзможна, следователно SVM не работи правилно в такъв случай
        * Тоест задължително условие е класовете да могат да бъдат разделени с права
    * XOR функцията е много хубава за класификатори (да проверим как работят)
    * ако измеренията са повече, правата става равнина
        * равнината е многомерна права
    * vectors са точките
    * support - крайните точки "подпират" полето и определят максималния gap
    * модела обръща най-много внимание на точките които са около границата (подпиращи точки/support vectors)
    * използва се за класификация по-често и за регресия по-рядко
        * при класификация модела казва кой клас е една нова точка
        * при регресия модела взима средно аритметично от всички предсказания
    * Ако класовете не могат да бъдат разделени, трябва да използваме регуляризация
    * SVM в sklearn използва регуляризация
    * Ако нямаме права която разделя класовете напълно, се избира възможно най-добрата права
    * Регуляризация която ползва SVM е само L2
    * Параметъра "C" се използва за регуляризация: ако е малка стойност означава голяма регуляризация
    * Как да преценим кога какъв параметър ни трябва?
        * от данните (ограничаване на избора)
        * търсим го с Grid Search
            * Grid Search се използва основно за регуляризация
    * Всеки един ML model има възможност за използване на регуляризация, използва се за:
        * да се справи добре модела
        * да бъде силен
        * да не underfit-ва
        * да не се справя твърде добре с трениращите данни (overfitting)
    * SVM работи бавно
        * сложността зависи от броя записи
        * в най-лошия случай работи като "броя samples на 3та степен"
        * за няколко стотин хиляди вече не работи (~10^5)
        * sklearn.svm.LinearSVM работи за повече (има оптимизации)
        * трябва ни да правим Grid Search, Bias-Variance tradeoff, но не можем да пробваме много стойности, защото няма да имаме толкова много време
    * Kernel trick
        * ако данните не не могат да се разделя с права (но могат с кръгче примерно) можем да намерим проекция която ги разделя
        * увеличаваме размерността на пространството поне с 1 (пример: от двумерно пространство създаваме тримерно)
            * добавя се един нов feature ако увеличим с 1
        * ако проектираме данни и пак се окажат в една равнина, модела ще избере най-добрата права (но това значи че такава проекция не съществува, т.е. не могат да бъдат разделени)
        * начина по който избираме проекцията се нарича mapping function (kernel)
            * вида на нашия kernel опрделя вида на трансформацията която използваме
        * основната идея е да разделим точките със хипер равнина
        * най-често използван kernel: Radial Basis Function (Gaussian)
            * ширината (стандартното отклонение) на гаусовото разпределение определя големината на kernel-a
                * малък kernel - голяма регуляризация (т.е. не можем да хванем всички точки)
                * голям kernel - малка регуляризация (можем да хванем повече точки, но ако е твърде голям прави overfitting)
                * параметъра "гама" - стандартното отклонение, подлежи на оптимизация (например чрез Grid Search)
                   * малка стойност - голяма регуляризация
                   * голям стойност - малка регуляризация
    * SVM е много чувствителен към скалирането на данните
        * не трябва да се използва преди да сме скалирали данните предварително
            * ако скалираме данните предварително се ограничава search space-a
            * сложността намалява (така работи библиотеката)
        * т.е. ако данните са нормализирани, много по-добре ще се работи с тях
* ако имаме много небалансирани класове е важно да направим стратификация при **train_test_split**
* K-Nearest Neighbors
    * наричаме го **lazy learner** защото нямаме функция която да оптимизира
    1. за всяка една точка изчисляваме разстоянието до всяка една друга точка
    2. следим кои са "k" на брой най-близки съседи
    3. сортираме разстоянията и взимаме най-малките ("к" на брой)
    4. От избраните съседи, предсказваме
        * при класификация използваме majority vote (т.е. от който клас има най-много)
        * при регресия използваме средното аритметично
    * не предсказва нищо конкретно, научава всяка една точка къде се намира
    * този модел научава всяка една точка къде се намира **наизуст**, което е предпоставка за overfitting
    * зависи от броя на точките
        * когато добавяме нова точка, трябва да изчислим нейното разстояние спрямо всяка една друга точка
        * всяко едно предсказване става все по-бавно
            * но има структури от данни които са оптимизирани да намират разстояние в пространството по бърз начин, например K-Dimensional Tree
    * параметъра "k" подлежи на оптимизация
    * важно е каква мярка ще използваме за измерване на разстояние
        * стандартно се използва еклидовата метрика за разстояние и работи идеално
        * може да използваме и други метрики за разстояние 
            * Minkowski distance
            * Manhattan distance
            * Taxicab distance
        * трябва да внимаваме ако границите между класовете са много "резки" тогава Manhattan distance няма да хване разликата, твърде грубо е за да я хване
    * ако имаме класификация, "k" трябва да не се дели точно на броя класове
        * пример за грешно използване: имаме 2 класа, и избираме k = 6 (може да стане така че да имаме 3 вида от единия клас и 3 от другия) и така няма как да работи majority vote.
    *  при много на брой съседи има по-голям шанс да получим реален majority vote
    *  при един на брой съсед, се получава диаграма на Вороной: https://en.wikipedia.org/wiki/Voronoi_diagram
    *  ако имаме точки (данни) които са твърде далеч от другите, това е проблем защото когато взимаме "k" най-близки съседи може да иммаме само от едната страна точки. Това е вид екстраполация. Ние не знаем какво би било в други посоки
* всички алгоритми които сме учили до момента, освен RANSAC предполагат че 
    * данните не съдържат много outlier-и и много аномални данни 
    * данните могат да бъдат апроксимирани с фукнция
    * данните са регулярни
    * има зависимост в данните
* Откриване на аномалии (Anomaly Detection)
    * Как работи RANSAC?
        1. Имаме някакви точки в многомерно пространство
        2. Някои от тях смятаме че са inliers, някои от тях смятаме че са outliers
        3. Искаме да намерим модел който изключва точките които са outliers
        * Това може да се направи като потърсим случайно подможество от точките
            1. Вземаме случайно подмножество от точките 
            2. Апроксимираме подмножеството с някакъв модел (примери: линейна регресия, Support Vector Regression, Decision Forest Regressor) - Фитираме
            3. Тестваме друг, нов модел с друго случайно подмножество от точките
            4. Сравняваме двата модела и виждаме каква е вариацията в тях
            5. Повтаряме горните стъпки много на брой пъти
            * Според статистиката от много на брой опити, ще уцелим този модел който дава най-добрите тестови резултати
            * Модела който дава най-добрите тестови резулатите, накрая е намерил точките които са само inliers
            * Колкото повече итерации имаме толкова по-добре
                * може да спрем да итерираме на определн брой итерации или когато достигнем определена точност (пример: Accuracy > 90%)
    * Търсене на аномалии е вид класификация
    * Novelty Detection е вид anomaly detection
        * тренираме само с нормални данни
        * аномалията се получава едва при тестване
        * предполагаме че аномалии могат да съществуват, но знаем че при трениращите данни нямаме аномалии
        * поставяме "граница" спрямо която всяко друго нещо което е извън нея ще бъде аномалия
    * при anomaly/novelty detection 
        * приемаме че всички данни са валидни
            * примери:
                * ракови клетки (имаме много на брой здрави пациенти и малко на брой болни пациенти). Въпреки че болните са малко тук, те ни интересуват
                * писма (имаме много на брой нормални писма и малко на брой спам писма). Въпреки че спам писмата са малко, те ни интересуват. Те са различни от другите (аномални), но тази разлика (отклонението от другите точки) е характерна за данните
    * За **откриване на аномалии** може да използваме **one-class SVM**
        * един kernel дава някаква "плътност" и според тази плътност може да разберем дали данните са аномални или не
        * докато тренираме модела, всяка една голяма разлика ще бъде приета като аномалия
        * и след като модела е разбрал кои са аномалиите може да предсказваме дали нови точки са нормални или не
        * параметри:
            * гама - ширина на kernel
            * ню/ни - "границата/прагът", вероятността да намерим точка която е много далеч
                * ако е голяма вероятността, само точки които са **много** далеч от централния kernel за да бъде класифицирани като аномалии
                * най-добре е този параметър да се оптимизира (например с Grid Search)
            * degree се използва само при polynomial kernel
            * може да не му се подават "labels" при .fit метода т.е. на този модел не му трябва да знае резултата
                * така работи unsupervised learning
                * правим предположения при създаването на модела с неговите параметри
    * когато използваме модели за търсене на аномалии, трябва да стратифицираме правилно данните
        * ако не можем, може да пробваме да използваме bootstrap resample
        * one-class SVM може да се изпозлва за outlier detection, но има много по-добри методи от него
* Outlier detection
    * имаме данни, но малка част от тях са "далеч" от повечето
    * може да бъдат сбъркани данни
    * т.е. тука игнорираме "аномалните" данни за разлика от "anomaly/novelty detection"
    * за откриване на outliers може да използваме RANSAC
* Можем да експортираме променливите в нашите модели и да ги ползваме някъде другаде (например в нов Jupyter Notebook). Това може да стане с **pickle** библиотеката
