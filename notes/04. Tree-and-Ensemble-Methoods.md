Decision tree
* can be used for classification or regression
	* root - top node (always a single root)
	* leaves - bottom nodes
	* getting an answer - path from the root to leaf
* biggest advantage: easy to interpret
* answer a series of yes/no questions to get the data model
	* similar to the way we decide what to do
* we can construct our own decision tree using if-statements
	* ML problem: construct the tree without involving "brain power"
* всеки един връх е въпрос
* всеки един възможен отговор на въпроса сочи към друг въпрос или решение
* разбираме че сме стигнали до решението когато нямаме повече въпроси да питаме
* имаме разделяне на данните което ни позволя като тръгнем по дървото да вземем решение
* iterate until every leaf node contains only one classification
	* to avoid overfitting => pruning (limit the max depth)
* objective function: maximize IG (Information Gain)
	* greedy algorithm
		* не винаги е гарантирано че ще намери най-доброто разделение
	* започваме от корена (root)
	* на всяка една стъпка решаваме как да разделим данните
		* избираме една колонка по която да разделим данните
			* разделяме ги на базата на класовете които трябва да върнем накрая (един от многото отговори)
			* колонката която избираме трябва да бъде с най-голям "information gain"
	* information gain - до колко една колонка е успяла да раздели по най-добър начин данните
		* до колко новите данни могат да бъдат класифицирани по-лесно от старите
	* difference between parent and child impurities
		* greater difference = more IG
	* Impurity measure
		* хипер параметър на алгоритъма
			* можем да направим cross-validation и да изберем най-добрата мярка
		* мярка за тва "до колко хаосът в една система е намалял"
		* most libraries implement binary decision trees
			* each node can have 0, 1 or 2 children
			* reasons:
				* simplicity
				* reducing the search space
		* 3 common impurity measures
			* Entropy - measure of classification uncertainty
				* probability 0 or 1 = no uncertainty
				* probability 0.5 = max uncertainty
				* ако ентропията намалява, нашата сигурност се увеличава
			* Gini index (similar to Entropy)
				* criterion to minimize the probability of misclassification
				* we usually use one of the measures, as they provide similar results
				* може да се срещне също като "gini coefficient"
			* Misclassification error
				* мярка за тва колко сме далеч от истината
				* Linear measure
					* 0 at p = {0;1}
					* max at p = 0.5
				* good for pruning a tree
				* worst for growing
	* model hyperparameters
		* ако оставим max_depth параметъра с default-ната стойност може да имаме голяма вариация (overfitting)
			* примерно решение: може да ограничим дървото да расте максимално на 3 нива
				* вкарваме bias, но намаляваме вариацията
			* как избираме правилната стойност за max_depth?
				* с cross-validation
		* max_features - максимален брой колонки
	* outputs
		* feature_importances
			* до колко всяка една колонка е успяла да разделя данните в дървото
			* до колко всяка една колонка е дала информация за данните
		* n_classes
			* брой класове
		* n_features
			* брой колонки
	* ако една променлива завършва с _ в sklearn, това означава че тя се е получила след обучение на модела
	* когато добавяме някой параметър към модел добавяме bias, правим bias-variance trade-off
	* Visualizing Decision Tree Boundaries
		* this method can be applied to all classifiers, not only trees
			* select 2 features (for 2D plot)
			* predict class values for a "mesh" of evenly-spaced samples
			* plot the test data and predicted values in different colors
		* при класификацията искаме да разберем каква е границата между класовете
			* формата на границата може да покаже колко добре се справя алгоритъма
			* countorf plot - тип визуализация която приема meshgrid и прави от него една цяла площ, вместо да "плотва" отделните точки
			* ако имаме по-малък max-depth самата функция става по-проста

Decision Forest
* обединява много дървета
* combination (ensambles) of decision trees
* idea
	* combine many weak learners (models that perform slightly better than random)
	* да накараме моделите да работят заедно
		* една система е повече от сумата от частите си
	* избираме случайно подмножество на данните
		* пример: тренираме върху първите 50%
	* всяко едно дърво тренира върху някакво подмножество на данните (bootstrap sample)
		* => имаме много на брой дървета, като всяко едно от тях е тренирано на различно подмножество от данните
		* когато поискаме да класифицираме една нова точка, питаме всеки един от моделите какъв е класът
		* агрегираме - използваме majority vote и ще класифицираме точката с този клас който е най-много предлаган
		* 100 дървета слаби да по-добри от 1 силно
	* Разлики от decision trees
		* Горите използват случайно подмножество на данните, докато дърветата използват всички данните
		* 100 дървета е много трудно да разберем какво правят
			* дори да ги визуализираме, няма да получим ясна представа какво се случва
	* Ползи
		* по-малка грешка (lower generalization error)
		* по-малко податливи на overfitting (less susceptible to overfitting)
			* нормално ограничаваме много дърветата от към max-depth => много трудно те могат да overfit-нат
		* по-малко hyperparameter-tuning
			* нормално няма значение с какви параметри са дърветата
			* това което има значение е БРОЯ на дърветата
				* защото сме ограничили max-depth
		* в повечето случаи дава най-добрите резултати от стандартните алгоритми за ML
			* стандартни алгоритми в ML - всички, без невронни мрежи

AdaBoost (Adaptive Boosting)
* друг начин да комбинираме няколко слаби модела в един по-силен
* boosting - да подобрим изпълнението на всеки един модел
* algorithm
	* train a weak learner on a random subset (without replacement) of the test data
	* get another random subset and add 50% of the previously misclassified samples
	* train another weak learner on the new random subset from the previous step + 50% of the previously missclassified samples
	* run the previous step for another weak learner
	* and so on...
	* combine all weak learners via majority voting (for classification)
		* обединяваме резултатите в един 
		* при регресия, обикновено вземаме средното аритметично
* тези алгоритми имат тенденция да overfit-ват
	* трябва внимателно да проверяваме variance-а
	* трябва да ограничим сложността на вътрешните алгоритми за да избегнем overfitting
		т.е. да имаме слаби алгоритми (малко по-добре от случайно работещи)
* идеяата за boosting моделите е една и съща
	* позволява на всеки един weak learner да се специализира
стъпки
	1. all samples have equal weight
		* minimize error function
	2. assing larger weight to missclassified samples, lower weights to correctly classified samples
		* focuses on misclassified samples
	3. perform the step 2 many times as needed
	4. combine all weak learners
	* Бележки:
		* тежестта, всеки път се увеличава
		* всеки алгоритъм работи там, където другите не могат
* AdaBoost е по-добър в повечето случаи
	* предсказва по-добре
* AdaBoost има висока вариация и намален bias	
	* как да разберем кой модел е по-добър?
		* cross-validation, model selection process

Допълнителни бележки
	* Не можем да визуализираме dataset с 15 измерения наведнъж
	* Decision tree може да се ползва и за регресия и класификация
	* Ако много неща варират заедно, не ни интересуват (Централна Гранична Теорема)
		* ако имаме много случайни променливи, сумата от тях наподобява специално разпределение (гаусово)
		* ако твърде много неща се променят, не ни интересуват
	* Невронните мрежи се тренират само върху много големи обеми данни
		* хората които не обичат невронни мрежи, нормално използват decision forests
	* GBoost (Gradient Boosting)
	* XGBoost (Extreme Gradient Boosting)
	* Колкото повече данни вижда един модел с фиксирана сложност, толкова по-малко вероятно е да overfit-не
	* Колкото по-малко данни вижда един модел, толкова повече може да си варира в тях
	* Има алгоритми които работят върху други алгоритми
		* примери: AdaBoost, RANSAC
	* Boosting - начин на работа, може да работи върху много алгоритми
	* Стъпки за тва как да изберем модел
		* тренираме алгоритми върху training set
		* правим model selection на базата на validation set
		* избираме най-добрия модел и го тестваме на testing set
	* Прост модел - голям bias (много предположения които вкарваме и ако имаме данни които са далече от тях няма да ги предскажем правилно), но по-малка вариация
	* Едно decision tree рядко ще свърши работа, нормално се използва decision forest
	* Decision jungle - вариант на decision forest, само че се избираме по-малко features, работи по-бързо
	* f1_score, precision, recall метриките работят само за класификация от 2 класа

Допълнителни ресурси
	* http://www.r2d3.us/visual-intro-to-machine-learning-part-1/
